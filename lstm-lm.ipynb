{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Import library","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nimport math\n\nimport datasets\n\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T06:27:25.375668Z","iopub.execute_input":"2024-12-26T06:27:25.377172Z","iopub.status.idle":"2024-12-26T06:27:29.868400Z","shell.execute_reply.started":"2024-12-26T06:27:25.377142Z","shell.execute_reply":"2024-12-26T06:27:29.867513Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ntorch.manual_seed(0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T06:27:32.400267Z","iopub.execute_input":"2024-12-26T06:27:32.400590Z","iopub.status.idle":"2024-12-26T06:27:32.461191Z","shell.execute_reply.started":"2024-12-26T06:27:32.400566Z","shell.execute_reply":"2024-12-26T06:27:32.460315Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x7c4c0652c8f0>"},"metadata":{}}],"execution_count":2},{"cell_type":"markdown","source":"## Dataset ","metadata":{}},{"cell_type":"code","source":"### Load the Dataset\ndataset = datasets.load_dataset('wikitext', 'wikitext-2-raw-v1')\nprint(dataset)\nprint(dataset['train'][88]['text'])\n\nfrom collections import Counter\nimport re\n\nclass Tokenizer:\n    def __init__(self):\n        self.pattern = re.compile(r'\\b\\w+\\b|[^\\w\\s]')\n        \n    def __call__(self, text):\n        return self.pattern.findall(text.lower())\n\nclass Vocab:\n    def __init__(self, min_freq=3):\n        self.stoi = {'<unk>': 0, '<eos>': 1}\n        self.itos = ['<unk>', '<eos>']\n        self.min_freq = min_freq\n        \n    def build(self, tokens):\n        counter = Counter([t for doc in tokens for t in doc])\n        for word, freq in counter.items():\n            if freq >= self.min_freq and word not in self.stoi:\n                self.stoi[word] = len(self.itos)\n                self.itos.append(word)\n    \n    def __len__(self):\n        return len(self.itos)\n        \n    def __getitem__(self, token):\n        return self.stoi.get(token, self.stoi['<unk>'])\n\ntokenizer = Tokenizer()\ntokenize_data = lambda example, tokenizer: {'tokens': tokenizer(example['text'])}\ntokenized_dataset = dataset.map(tokenize_data, remove_columns=['text'], fn_kwargs={'tokenizer': tokenizer})\n\nvocab = Vocab(min_freq=3)\nvocab.build(tokenized_dataset['train']['tokens'])\n\nprint(len(vocab))                         # total number words in the vocabulary\n\ndef get_data(dataset, vocab, batch_size):\n    data = []                                                       # Merge everything into one gigantic document that we wish to model (all the tokens)\n    for example in dataset:\n        if example['tokens']:                                       # if the example has tokens (not empty)\n            tokens = example['tokens'].append('<eos>')              # append <eos> at the end of the sentence\n            tokens = [vocab[token] for token in example['tokens']]  # convert tokens to indices\n            data.extend(tokens)                                     # append tokens to data\n    data = torch.LongTensor(data)                                   # convert data to tensor\n    num_batches = data.shape[0] // batch_size \n    data = data[:num_batches * batch_size]                         # We only need the first num_batches * batch_size elements\n    data = data.view(batch_size, num_batches)            # Perceive the data as a matrix of batch_size rows and num_batches columns\n    return data\n\n#Notice that train_data[:, i] is the batch of next tokens for train_data[:, i - 1] \nbatch_size = 256\ntrain_data = get_data(tokenized_dataset['train'], vocab, batch_size)\nvalid_data = get_data(tokenized_dataset['validation'], vocab, batch_size)\ntest_data = get_data(tokenized_dataset['test'], vocab, batch_size)\n\ndef get_batch(data, seq_len, num_batches, idx):\n    src = data[:, idx:idx+seq_len]                   \n    target = data[:, idx+1:idx+seq_len+1]             # The target is the src shifted by one batch\n    return src, target","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T06:27:35.941190Z","iopub.execute_input":"2024-12-26T06:27:35.941474Z","iopub.status.idle":"2024-12-26T06:27:47.599760Z","shell.execute_reply.started":"2024-12-26T06:27:35.941451Z","shell.execute_reply":"2024-12-26T06:27:47.598878Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d42e522461041d487f99fcb7f06dc25"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/733k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2fc3613264543a19911aa3f3003679b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/6.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7cc20874297e473daee23400229e10d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/657k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b4bf841f1ce472e8ddcdc35ff1d6616"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e66b2cc45c649f6b2e1d9c8f2efb62a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3196c14c223f4f82a0efef56c64aa4d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88b527778ad949a293ec0c212845f80a"}},"metadata":{}},{"name":"stdout","text":"DatasetDict({\n    test: Dataset({\n        features: ['text'],\n        num_rows: 4358\n    })\n    train: Dataset({\n        features: ['text'],\n        num_rows: 36718\n    })\n    validation: Dataset({\n        features: ['text'],\n        num_rows: 3760\n    })\n})\n This ammunition , and that which I brought with me , was rapidly prepared for use at the Laboratory established at the Little Rock Arsenal for that purpose . As illustrating as the pitiful scarcity of material in the country , the fact may be stated that it was found necessary to use public documents of the State Library for cartridge paper . Gunsmiths were employed or conscripted , tools purchased or impressed , and the repair of the damaged guns I brought with me and about an equal number found at Little Rock commenced at once . But , after inspecting the work and observing the spirit of the men I decided that a garrison 500 strong could hold out against Fitch and that I would lead the remainder - about 1500 - to Gen 'l Rust as soon as shotguns and rifles could be obtained from Little Rock instead of pikes and lances , with which most of them were armed . Two days elapsed before the change could be effected . \" \n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4358 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8010e5a3bf244a7489365c09c54063ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/36718 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0149e46290545a08c4d417edd320320"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3760 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3b0e7acae244fa8ab946d201ab5355c"}},"metadata":{}},{"name":"stdout","text":"29482\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## LSTM","metadata":{}},{"cell_type":"code","source":"class LSTM(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, tie_weights):\n        super().__init__()\n        self.num_layers = num_layers\n        self.hidden_dim = hidden_dim\n        self.embedding_dim = embedding_dim\n\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, dropout=dropout_rate, batch_first=True)\n        self.dropout = nn.Dropout(dropout_rate)\n        self.fc = nn.Linear(hidden_dim, vocab_size)\n        \n        if tie_weights:\n            assert embedding_dim == hidden_dim, 'If tying weights then embedding_dim must equal hidden_dim'\n            self.embedding.weight = self.fc.weight\n        self.init_weights()\n\n    def forward(self, src, hidden):\n        embedding = self.dropout(self.embedding(src))\n        output, hidden = self.lstm(embedding, hidden)          \n        output = self.dropout(output) \n        prediction = self.fc(output)\n        return prediction, hidden\n\n    def init_weights(self):\n        init_range_emb = 0.1\n        init_range_other = 1/math.sqrt(self.hidden_dim)\n        self.embedding.weight.data.uniform_(-init_range_emb, init_range_emb)\n        self.fc.weight.data.uniform_(-init_range_other, init_range_other)\n        self.fc.bias.data.zero_()\n        for i in range(self.num_layers):\n            self.lstm.all_weights[i][0] = torch.FloatTensor(self.embedding_dim,\n                    self.hidden_dim).uniform_(-init_range_other, init_range_other) \n            self.lstm.all_weights[i][1] = torch.FloatTensor(self.hidden_dim, \n                    self.hidden_dim).uniform_(-init_range_other, init_range_other) \n\n    def init_hidden(self, batch_size, device):\n        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n        cell = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n        return hidden, cell\n\n    # We don't learn the hidden state so we can detach it from the computation graph\n    def detach_hidden(self, hidden):\n        hidden, cell = hidden\n        hidden = hidden.detach()\n        cell = cell.detach()\n        return hidden, cell\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T06:28:09.140511Z","iopub.execute_input":"2024-12-26T06:28:09.140884Z","iopub.status.idle":"2024-12-26T06:28:09.149288Z","shell.execute_reply.started":"2024-12-26T06:28:09.140855Z","shell.execute_reply":"2024-12-26T06:28:09.148580Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"vocab_size = len(vocab)\nembedding_dim = 128             # 400 in the paper\nhidden_dim = 256                # 1150 in the paper\nnum_layers = 2                   # 3 in the paper\ndropout_rate = 0.65              \ntie_weights = False                  \nlr = 1e-3                        # They used 30 and a different optimizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T06:29:06.663934Z","iopub.execute_input":"2024-12-26T06:29:06.664227Z","iopub.status.idle":"2024-12-26T06:29:06.668434Z","shell.execute_reply.started":"2024-12-26T06:29:06.664205Z","shell.execute_reply":"2024-12-26T06:29:06.667389Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"model = LSTM(vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, tie_weights).to(device)\noptimizer = optim.Adam(model.parameters(), lr=lr)\ncriterion = nn.CrossEntropyLoss()\nnum_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f'The model has {num_params:,} trainable parameters')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T06:29:12.060996Z","iopub.execute_input":"2024-12-26T06:29:12.061341Z","iopub.status.idle":"2024-12-26T06:29:12.235296Z","shell.execute_reply.started":"2024-12-26T06:29:12.061314Z","shell.execute_reply":"2024-12-26T06:29:12.234385Z"}},"outputs":[{"name":"stdout","text":"The model has 12,272,170 trainable parameters\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"## Train","metadata":{}},{"cell_type":"code","source":"def train(model, data, optimizer, criterion, batch_size, seq_len, clip, device):\n    \n    epoch_loss = 0\n    model.train()\n    # drop all batches that are not a multiple of seq_len\n    num_batches = data.shape[-1]\n    data = data[:, :num_batches - (num_batches -1) % seq_len]\n    num_batches = data.shape[-1]\n\n    hidden = model.init_hidden(batch_size, device)\n    \n    for idx in tqdm(range(0, num_batches - 1, seq_len), desc='Training: ',leave=False):  # The last batch can't be a src\n        optimizer.zero_grad()\n        hidden = model.detach_hidden(hidden)\n\n        src, target = get_batch(data, seq_len, num_batches, idx)\n        src, target = src.to(device), target.to(device)\n        batch_size = src.shape[0]\n        prediction, hidden = model(src, hidden)                 # model output\n\n        prediction = prediction.reshape(batch_size * seq_len, -1)   \n        target = target.reshape(-1)\n        loss = criterion(prediction, target)\n        \n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n        optimizer.step()\n        epoch_loss += loss.item() * seq_len\n    return epoch_loss / num_batches\ndef evaluate(model, data, criterion, batch_size, seq_len, device):\n\n    epoch_loss = 0\n    model.eval()\n    num_batches = data.shape[-1]\n    data = data[:, :num_batches - (num_batches -1) % seq_len]\n    num_batches = data.shape[-1]\n\n    hidden = model.init_hidden(batch_size, device)\n\n    with torch.no_grad():\n        for idx in range(0, num_batches - 1, seq_len):\n            hidden = model.detach_hidden(hidden)\n            src, target = get_batch(data, seq_len, num_batches, idx)\n            src, target = src.to(device), target.to(device)\n            batch_size= src.shape[0]\n\n            prediction, hidden = model(src, hidden)\n            prediction = prediction.reshape(batch_size * seq_len, -1)\n            target = target.reshape(-1)\n\n            loss = criterion(prediction, target)\n            epoch_loss += loss.item() * seq_len\n    return epoch_loss / num_batches\nn_epochs = 50\nseq_len = 50\nclip = 0.25\nsaved = False\n\nlr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=0)\n\nif saved:\n    model.load_state_dict(torch.load('best-val-lstm_lm.pt',  map_location=device))\n    test_loss = evaluate(model, test_data, criterion, batch_size, seq_len, device)\n    print(f'Test Perplexity: {math.exp(test_loss):.3f}')\nelse:\n    best_valid_loss = float('inf')\n\n    for epoch in range(n_epochs):\n        print(f\"Epoch {epoch+1}:\")\n        train_loss = train(model, train_data, optimizer, criterion, batch_size, seq_len, clip, device)\n        valid_loss = evaluate(model, valid_data, criterion, batch_size, seq_len, device)\n        \n        lr_scheduler.step(valid_loss)\n\n        if valid_loss < best_valid_loss:\n            best_valid_loss = valid_loss\n            torch.save(model.state_dict(), 'best-lstm.pt')\n\n        print(f'\\tTrain Perplexity: {math.exp(train_loss):.3f}')\n        print(f'\\tValid Perplexity: {math.exp(valid_loss):.3f}')\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T06:30:47.160645Z","iopub.execute_input":"2024-12-26T06:30:47.160966Z","iopub.status.idle":"2024-12-26T06:49:51.439256Z","shell.execute_reply.started":"2024-12-26T06:30:47.160940Z","shell.execute_reply":"2024-12-26T06:49:51.438574Z"}},"outputs":[{"name":"stdout","text":"Epoch 1:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 528.978\n\tValid Perplexity: 366.932\nEpoch 2:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 457.937\n\tValid Perplexity: 319.140\nEpoch 3:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 403.885\n\tValid Perplexity: 287.497\nEpoch 4:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 366.710\n\tValid Perplexity: 265.202\nEpoch 5:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 338.905\n\tValid Perplexity: 248.858\nEpoch 6:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 316.841\n\tValid Perplexity: 235.841\nEpoch 7:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 298.527\n\tValid Perplexity: 224.738\nEpoch 8:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 283.008\n\tValid Perplexity: 215.826\nEpoch 9:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 270.548\n\tValid Perplexity: 209.334\nEpoch 10:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 260.080\n\tValid Perplexity: 204.226\nEpoch 11:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 250.211\n\tValid Perplexity: 198.541\nEpoch 12:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 241.864\n\tValid Perplexity: 195.256\nEpoch 13:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 234.632\n\tValid Perplexity: 190.461\nEpoch 14:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 226.695\n\tValid Perplexity: 184.692\nEpoch 15:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 219.543\n\tValid Perplexity: 183.149\nEpoch 16:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 214.445\n\tValid Perplexity: 182.493\nEpoch 17:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 208.958\n\tValid Perplexity: 177.829\nEpoch 18:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 203.560\n\tValid Perplexity: 175.473\nEpoch 19:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 199.330\n\tValid Perplexity: 171.532\nEpoch 20:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 195.444\n\tValid Perplexity: 169.723\nEpoch 21:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 191.700\n\tValid Perplexity: 167.716\nEpoch 22:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 187.791\n\tValid Perplexity: 166.012\nEpoch 23:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 184.281\n\tValid Perplexity: 164.676\nEpoch 24:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 181.670\n\tValid Perplexity: 162.344\nEpoch 25:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 177.822\n\tValid Perplexity: 162.900\nEpoch 26:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 173.469\n\tValid Perplexity: 158.508\nEpoch 27:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 171.481\n\tValid Perplexity: 157.032\nEpoch 28:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 170.100\n\tValid Perplexity: 156.330\nEpoch 29:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 168.621\n\tValid Perplexity: 155.790\nEpoch 30:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 167.039\n\tValid Perplexity: 154.691\nEpoch 31:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 165.877\n\tValid Perplexity: 154.517\nEpoch 32:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 164.253\n\tValid Perplexity: 153.787\nEpoch 33:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 163.521\n\tValid Perplexity: 155.263\nEpoch 34:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 161.194\n\tValid Perplexity: 152.615\nEpoch 35:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 160.249\n\tValid Perplexity: 152.294\nEpoch 36:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 159.531\n\tValid Perplexity: 151.868\nEpoch 37:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 158.856\n\tValid Perplexity: 151.807\nEpoch 38:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 158.027\n\tValid Perplexity: 151.171\nEpoch 39:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 157.612\n\tValid Perplexity: 151.053\nEpoch 40:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 157.322\n\tValid Perplexity: 150.814\nEpoch 41:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 157.128\n\tValid Perplexity: 150.712\nEpoch 42:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 156.685\n\tValid Perplexity: 150.589\nEpoch 43:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 156.338\n\tValid Perplexity: 150.361\nEpoch 44:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 156.085\n\tValid Perplexity: 150.229\nEpoch 45:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 155.768\n\tValid Perplexity: 150.242\nEpoch 46:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 155.528\n\tValid Perplexity: 150.142\nEpoch 47:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 155.267\n\tValid Perplexity: 149.962\nEpoch 48:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 155.081\n\tValid Perplexity: 149.937\nEpoch 49:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 154.839\n\tValid Perplexity: 149.621\nEpoch 50:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 154.961\n\tValid Perplexity: 149.679\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"## Test\n","metadata":{}},{"cell_type":"code","source":"model.load_state_dict(torch.load('/kaggle/working/best-lstm.pt',  map_location=device))\ntest_loss = evaluate(model, test_data, criterion, batch_size, seq_len, device)\nprint(f'Test Perplexity: {math.exp(test_loss):.3f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T06:50:06.442825Z","iopub.execute_input":"2024-12-26T06:50:06.443111Z","iopub.status.idle":"2024-12-26T06:50:07.405366Z","shell.execute_reply.started":"2024-12-26T06:50:06.443086Z","shell.execute_reply":"2024-12-26T06:50:07.404561Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-15-9515d9abdcbe>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('/kaggle/working/best-lstm.pt',  map_location=device))\n","output_type":"stream"},{"name":"stdout","text":"Test Perplexity: 140.080\n","output_type":"stream"}],"execution_count":15}]}