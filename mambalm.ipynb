{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10296894,"sourceType":"datasetVersion","datasetId":6373214}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom datasets import Dataset, DatasetDict\nimport re\n# Đọc dữ liệu từ file parquet\ntrain_df = pd.read_parquet(\"/kaggle/input/mambaa/train-00000-of-00001 (2).parquet\")\ntrain_df = train_df[train_df['text'].str.strip() != \"\"]\nval_df = pd.read_parquet(\"/kaggle/input/mambaa/validation-00000-of-00001 (1).parquet\")\nval_df = val_df[val_df['text'].str.strip() != \"\"]\ntest_df = pd.read_parquet(\"/kaggle/input/mambaa/test-00000-of-00001 (3).parquet\")\ntest_df = test_df[test_df['text'].str.strip() != \"\"]\n\ndef remove_special_characters(text):\n    return re.sub(r\"[^A-Za-z0-9\\s.,!?']\", '', text)\n\n# Áp dụng hàm vào cột 'text'\ntrain_df['text'] = train_df['text'].apply(remove_special_characters)\nval_df['text'] = val_df['text'].apply(remove_special_characters)\ntest_df['text'] = test_df['text'].apply(remove_special_characters)\n\n# Tạo Dataset từ DataFrame\ntrain_dataset = Dataset.from_pandas(train_df)\nval_dataset = Dataset.from_pandas(val_df)\ntest_dataset = Dataset.from_pandas(test_df)\n\n# Tạo DatasetDict\ndataset_dict = DatasetDict({\n    \"train\": train_dataset,\n    \"validation\": val_dataset,\n    \"test\": test_dataset\n})\n\nprint(dataset_dict)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T13:46:41.939648Z","iopub.execute_input":"2024-12-26T13:46:41.939957Z","iopub.status.idle":"2024-12-26T13:46:44.040446Z","shell.execute_reply.started":"2024-12-26T13:46:41.939930Z","shell.execute_reply":"2024-12-26T13:46:44.039653Z"}},"outputs":[{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['text', '__index_level_0__'],\n        num_rows: 23767\n    })\n    validation: Dataset({\n        features: ['text', '__index_level_0__'],\n        num_rows: 2461\n    })\n    test: Dataset({\n        features: ['text', '__index_level_0__'],\n        num_rows: 2891\n    })\n})\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from transformers import GPT2Tokenizer\n\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n# Gán EOS token làm pad token.\ntokenizer.pad_token = tokenizer.eos_token\n\ndef tokenize_function(examples):\n    # Add special tokens and return token_type_ids=False since it's language modeling\n    return tokenizer(\n        examples[\"text\"],\n        padding=True,\n        truncation=True,\n        max_length=50,\n        return_tensors=None,  # Don't force return tensors here - DataLoader will handle it\n        return_special_tokens_mask=True\n    )\n\ntokenized_datasets = dataset_dict.map(tokenize_function, batched=True)\n\n# Remove the 'text' column and other unnecessary columns\ntokenized_datasets = tokenized_datasets.remove_columns([col for col in tokenized_datasets[\"train\"].column_names if col not in [\"input_ids\", \"attention_mask\"]])\n\nprint(tokenized_datasets)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T13:51:40.028969Z","iopub.execute_input":"2024-12-26T13:51:40.029484Z","iopub.status.idle":"2024-12-26T13:52:08.266249Z","shell.execute_reply.started":"2024-12-26T13:51:40.029458Z","shell.execute_reply":"2024-12-26T13:52:08.265552Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f89e16da86a437eb46782025971ce95"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8ea1ce7a842419a8c07b47ecf721ac6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1a0dfa32a1446fe9bbd6938ebb596a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0af5e9f3177e46839f1fc8b1083508ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e079c8b78e394394b08c6e2e7e0ff043"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/23767 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67d3375644d443069851bb1cea32a394"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2461 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a597d74814648d1bef243e7d4ded54e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2891 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b51a3a7ef2f74509a6496fce896561fb"}},"metadata":{}},{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'attention_mask'],\n        num_rows: 23767\n    })\n    validation: Dataset({\n        features: ['input_ids', 'attention_mask'],\n        num_rows: 2461\n    })\n    test: Dataset({\n        features: ['input_ids', 'attention_mask'],\n        num_rows: 2891\n    })\n})\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"pip install mamba_ssm triton","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T13:52:38.462016Z","iopub.execute_input":"2024-12-26T13:52:38.462364Z","iopub.status.idle":"2024-12-26T13:55:21.101044Z","shell.execute_reply.started":"2024-12-26T13:52:38.462335Z","shell.execute_reply":"2024-12-26T13:55:21.099955Z"}},"outputs":[{"name":"stdout","text":"Collecting mamba_ssm\n  Downloading mamba_ssm-2.2.4.tar.gz (91 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.8/91.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nCollecting triton\n  Using cached triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from mamba_ssm) (2.4.1+cu121)\nRequirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from mamba_ssm) (1.11.1.3)\nRequirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from mamba_ssm) (0.8.0)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from mamba_ssm) (4.44.2)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from mamba_ssm) (24.1)\nRequirement already satisfied: setuptools>=61.0.0 in /usr/local/lib/python3.10/dist-packages (from mamba_ssm) (71.0.4)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton) (3.16.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->mamba_ssm) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->mamba_ssm) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->mamba_ssm) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->mamba_ssm) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->mamba_ssm) (2024.6.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba_ssm) (0.24.7)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba_ssm) (1.26.4)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba_ssm) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba_ssm) (2024.9.11)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->mamba_ssm) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba_ssm) (0.4.5)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba_ssm) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba_ssm) (4.66.5)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->mamba_ssm) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mamba_ssm) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mamba_ssm) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mamba_ssm) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mamba_ssm) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->mamba_ssm) (1.3.0)\nUsing cached triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\nBuilding wheels for collected packages: mamba_ssm\n  Building wheel for mamba_ssm (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for mamba_ssm: filename=mamba_ssm-2.2.4-cp310-cp310-linux_x86_64.whl size=323653202 sha256=c6edff068928b4ceacc6e820a163908c02294fe01dbcc05e95ed4530a5f81e77\n  Stored in directory: /root/.cache/pip/wheels/aa/af/c7/fb77bfcd94bd3e052545033449d8c47dc97222d79c39c5bc67\nSuccessfully built mamba_ssm\nInstalling collected packages: triton, mamba_ssm\nSuccessfully installed mamba_ssm-2.2.4 triton-3.1.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from mamba_ssm.models.config_mamba import MambaConfig\nfrom mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel\nimport torch\n\n# Cấu hình mô hình\nconfig = MambaConfig(\n    d_model=128,\n    n_layer=6,\n    vocab_size=tokenizer.vocab_size,\n    ssm_cfg={},\n    rms_norm=True,\n    fused_add_norm=True,\n    residual_in_fp32=True\n)\n\n# Khởi tạo mô hình\nmodel = MambaLMHeadModel(config).to(\"cuda\")\n\n# Định nghĩa optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\nbatch_size=16","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T13:56:06.781028Z","iopub.execute_input":"2024-12-26T13:56:06.781382Z","iopub.status.idle":"2024-12-26T13:56:07.094128Z","shell.execute_reply.started":"2024-12-26T13:56:06.781351Z","shell.execute_reply":"2024-12-26T13:56:07.093170Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom tqdm.auto import tqdm\n\n# Tạo DataLoader\nfrom transformers import DataCollatorWithPadding\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\ntrain_dataloader = DataLoader(\n    tokenized_datasets[\"train\"],\n    shuffle=True,\n    batch_size=batch_size,\n    collate_fn=data_collator,\n)\nval_dataloader = DataLoader(\n    tokenized_datasets[\"validation\"],\n    batch_size=batch_size,\n    collate_fn=data_collator,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T13:56:08.266575Z","iopub.execute_input":"2024-12-26T13:56:08.267026Z","iopub.status.idle":"2024-12-26T13:56:08.273661Z","shell.execute_reply.started":"2024-12-26T13:56:08.266984Z","shell.execute_reply":"2024-12-26T13:56:08.272662Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom tqdm.auto import tqdm\nimport torch.nn.functional as F\nimport math\n# Vòng lặp huấn luyện\n# Ensure GPUs are available\nif torch.cuda.is_available() and torch.cuda.device_count() > 1:\n    print(f\"Using {torch.cuda.device_count()} GPUs for training.\")\n    model = torch.nn.DataParallel(model)\nelse:\n    print(\"Using a single GPU or CPU for training.\")\nnum_epochs=20\n# Move model to GPU(s)\nmodel = model.to(\"cuda\")\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss = 0\n    for batch in tqdm(train_dataloader):\n        optimizer.zero_grad()\n        input_ids = batch[\"input_ids\"].to(\"cuda\")\n        outputs = model(input_ids)\n\n        # Shift tokens\n        shifted_input_ids = input_ids[:, 1:].clone()\n        shifted_logits = outputs.logits[:, :-1, :].contiguous()\n\n        loss = F.cross_entropy(\n            shifted_logits.view(-1, shifted_logits.size(-1)),\n            shifted_input_ids.reshape(-1),\n            ignore_index=tokenizer.pad_token_id\n        )\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n\n    avg_train_loss = total_loss / len(train_dataloader)\n    train_perplexity = math.exp(avg_train_loss)\n    print(f\"Epoch {epoch+1}: Average training loss: {avg_train_loss}, Training perplexity: {train_perplexity}\")\n\n    # Đánh giá trên tập validation\n    model.eval()\n    total_eval_loss = 0\n    with torch.no_grad():\n        for batch in val_dataloader:\n            input_ids = batch[\"input_ids\"].to(\"cuda\")\n            outputs = model(input_ids)\n\n            shifted_input_ids = input_ids[:, 1:].clone()\n            shifted_logits = outputs.logits[:, :-1, :].contiguous()\n\n            loss = F.cross_entropy(\n                shifted_logits.view(-1, shifted_logits.size(-1)),\n                shifted_input_ids.reshape(-1),\n                ignore_index=tokenizer.pad_token_id\n            )\n            total_eval_loss += loss.item()\n\n    avg_val_loss = total_eval_loss / len(val_dataloader)\n    val_perplexity = math.exp(avg_val_loss)\n    print(f\"Epoch {epoch+1}: Average validation loss: {avg_val_loss}, Validation perplexity: {val_perplexity}\")\n\n# Lưu mô hình\nmodel.save_pretrained(\"mamba_wikitext2\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}