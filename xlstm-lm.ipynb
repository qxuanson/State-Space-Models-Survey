{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Import library","metadata":{}},{"cell_type":"code","source":"!pip install xlstm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-26T03:29:51.483622Z","iopub.execute_input":"2024-12-26T03:29:51.484010Z","iopub.status.idle":"2024-12-26T03:29:57.000525Z","shell.execute_reply.started":"2024-12-26T03:29:51.483978Z","shell.execute_reply":"2024-12-26T03:29:56.999533Z"}},"outputs":[{"name":"stdout","text":"Collecting xlstm\n  Downloading xlstm-2.0.1-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from xlstm) (2.4.1+cu121)\nRequirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from xlstm) (0.8.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xlstm) (1.26.4)\nRequirement already satisfied: omegaconf in /usr/local/lib/python3.10/dist-packages (from xlstm) (2.3.0)\nRequirement already satisfied: opt_einsum in /usr/local/lib/python3.10/dist-packages (from xlstm) (3.3.0)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from xlstm) (4.44.2)\nCollecting reportlab (from xlstm)\n  Downloading reportlab-4.2.5-py3-none-any.whl.metadata (1.5 kB)\nCollecting joypy (from xlstm)\n  Downloading joypy-0.2.6-py2.py3-none-any.whl.metadata (812 bytes)\nRequirement already satisfied: ipykernel in /usr/local/lib/python3.10/dist-packages (from xlstm) (5.5.6)\nRequirement already satisfied: dacite in /usr/local/lib/python3.10/dist-packages (from xlstm) (1.8.1)\nCollecting ftfy (from xlstm)\n  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from xlstm) (1.11.1.3)\nRequirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from xlstm) (0.24.7)\nRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from xlstm) (13.8.1)\nRequirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (from xlstm) (0.19.1)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from xlstm) (4.66.5)\nRequirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (from xlstm) (0.12.2)\nINFO: pip is looking at multiple versions of xlstm to determine which version is compatible with other requirements. This could take a while.\nCollecting xlstm\n  Downloading xlstm-2.0.0-py3-none-any.whl.metadata (20 kB)\nDownloading xlstm-2.0.0-py3-none-any.whl (89 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.8/89.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: xlstm\nSuccessfully installed xlstm-2.0.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nimport math\n\nimport datasets\n\nfrom tqdm import tqdm\nfrom omegaconf import OmegaConf\nfrom dacite import from_dict\nfrom dacite import Config as DaciteConfig\nfrom xlstm import xLSTMLMModel, xLSTMLMModelConfig","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T03:29:57.001686Z","iopub.execute_input":"2024-12-26T03:29:57.001923Z","iopub.status.idle":"2024-12-26T03:30:02.094980Z","shell.execute_reply.started":"2024-12-26T03:29:57.001902Z","shell.execute_reply":"2024-12-26T03:30:02.094079Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ntorch.manual_seed(0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T03:30:02.096539Z","iopub.execute_input":"2024-12-26T03:30:02.096948Z","iopub.status.idle":"2024-12-26T03:30:02.106504Z","shell.execute_reply.started":"2024-12-26T03:30:02.096912Z","shell.execute_reply":"2024-12-26T03:30:02.105738Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x7dc8548a4a70>"},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"## Dataset ","metadata":{}},{"cell_type":"code","source":"### Load the Dataset\ndataset = datasets.load_dataset('wikitext', 'wikitext-2-raw-v1')\nprint(dataset)\nprint(dataset['train'][88]['text'])\n\nfrom collections import Counter\nimport re\n\nclass Tokenizer:\n    def __init__(self):\n        self.pattern = re.compile(r'\\b\\w+\\b|[^\\w\\s]')\n        \n    def __call__(self, text):\n        return self.pattern.findall(text.lower())\n\nclass Vocab:\n    def __init__(self, min_freq=3):\n        self.stoi = {'<unk>': 0, '<eos>': 1}\n        self.itos = ['<unk>', '<eos>']\n        self.min_freq = min_freq\n        \n    def build(self, tokens):\n        counter = Counter([t for doc in tokens for t in doc])\n        for word, freq in counter.items():\n            if freq >= self.min_freq and word not in self.stoi:\n                self.stoi[word] = len(self.itos)\n                self.itos.append(word)\n    \n    def __len__(self):\n        return len(self.itos)\n        \n    def __getitem__(self, token):\n        return self.stoi.get(token, self.stoi['<unk>'])\n\ntokenizer = Tokenizer()\ntokenize_data = lambda example, tokenizer: {'tokens': tokenizer(example['text'])}\ntokenized_dataset = dataset.map(tokenize_data, remove_columns=['text'], fn_kwargs={'tokenizer': tokenizer})\n\nvocab = Vocab(min_freq=3)\nvocab.build(tokenized_dataset['train']['tokens'])\n\nprint(len(vocab))                         # total number words in the vocabulary\n\ndef get_data(dataset, vocab, batch_size):\n    data = []                                                       # Merge everything into one gigantic document that we wish to model (all the tokens)\n    for example in dataset:\n        if example['tokens']:                                       # if the example has tokens (not empty)\n            tokens = example['tokens'].append('<eos>')              # append <eos> at the end of the sentence\n            tokens = [vocab[token] for token in example['tokens']]  # convert tokens to indices\n            data.extend(tokens)                                     # append tokens to data\n    data = torch.LongTensor(data)                                   # convert data to tensor\n    num_batches = data.shape[0] // batch_size \n    data = data[:num_batches * batch_size]                         # We only need the first num_batches * batch_size elements\n    data = data.view(batch_size, num_batches)            # Perceive the data as a matrix of batch_size rows and num_batches columns\n    return data\n\n#Notice that train_data[:, i] is the batch of next tokens for train_data[:, i - 1] \nbatch_size = 256\ntrain_data = get_data(tokenized_dataset['train'], vocab, batch_size)\nvalid_data = get_data(tokenized_dataset['validation'], vocab, batch_size)\ntest_data = get_data(tokenized_dataset['test'], vocab, batch_size)\n\ndef get_batch(data, seq_len, num_batches, idx):\n    src = data[:, idx:idx+seq_len]                   \n    target = data[:, idx+1:idx+seq_len+1]             # The target is the src shifted by one batch\n    return src, target","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T03:30:02.107564Z","iopub.execute_input":"2024-12-26T03:30:02.107817Z","iopub.status.idle":"2024-12-26T03:30:17.223086Z","shell.execute_reply.started":"2024-12-26T03:30:02.107796Z","shell.execute_reply":"2024-12-26T03:30:17.222217Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22ca8ffdf94f4f43a6274116a1dc943f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/733k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29870f3d221d4a31af37e1f80711de10"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/6.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16dd04f71e704ab0ad87d8c7ef918fe2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/657k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5361a1f7202407a95432c758ff8b73e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ada2e6be64ee4c0290f4fdad3e790a02"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10e3fedccbc54581871c428c3572ccaa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03864d3d6b1b4c16af677e14c9469756"}},"metadata":{}},{"name":"stdout","text":"DatasetDict({\n    test: Dataset({\n        features: ['text'],\n        num_rows: 4358\n    })\n    train: Dataset({\n        features: ['text'],\n        num_rows: 36718\n    })\n    validation: Dataset({\n        features: ['text'],\n        num_rows: 3760\n    })\n})\n This ammunition , and that which I brought with me , was rapidly prepared for use at the Laboratory established at the Little Rock Arsenal for that purpose . As illustrating as the pitiful scarcity of material in the country , the fact may be stated that it was found necessary to use public documents of the State Library for cartridge paper . Gunsmiths were employed or conscripted , tools purchased or impressed , and the repair of the damaged guns I brought with me and about an equal number found at Little Rock commenced at once . But , after inspecting the work and observing the spirit of the men I decided that a garrison 500 strong could hold out against Fitch and that I would lead the remainder - about 1500 - to Gen 'l Rust as soon as shotguns and rifles could be obtained from Little Rock instead of pikes and lances , with which most of them were armed . Two days elapsed before the change could be effected . \" \n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4358 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c5f280f5fec4b2494538e95c6464789"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/36718 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f314013c8ea5421298d6810de1e14a89"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3760 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f33717e475e46ca98589a6bab6330cb"}},"metadata":{}},{"name":"stdout","text":"29482\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## xLSTM","metadata":{}},{"cell_type":"code","source":"### Define xLSTM Configuration\nxlstm_cfg = \"\"\" \nvocab_size: 29482\nmlstm_block:\n  mlstm:\n    conv1d_kernel_size: 8\n    qkv_proj_blocksize: 8\n    num_heads: 8\nslstm_block:\n  slstm:\n    backend: cuda\n    num_heads: 8\n    conv1d_kernel_size: 8\n    bias_init: powerlaw_blockdependent\n  feedforward:\n    proj_factor: 1.3\n    act_fn: gelu\ncontext_length: 256\nnum_blocks: 8\nembedding_dim: 128\nslstm_at: [1]\n\"\"\"\ncfg = OmegaConf.create(xlstm_cfg)\ncfg = from_dict(data_class=xLSTMLMModelConfig, data=OmegaConf.to_container(cfg), config=DaciteConfig(strict=True))\nxlstm_stack = xLSTMLMModel(cfg)\nvocab_size = len(vocab)                 \nlr = 1e-3                        # They used 30 and a different optimizer\nmodel = xlstm_stack.to(device)\noptimizer = optim.Adam(model.parameters(), lr=lr)\ncriterion = nn.CrossEntropyLoss()\nnum_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f'The model has {num_params:,} trainable parameters')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T03:30:17.224080Z","iopub.execute_input":"2024-12-26T03:30:17.224440Z","iopub.status.idle":"2024-12-26T03:30:52.551290Z","shell.execute_reply.started":"2024-12-26T03:30:17.224407Z","shell.execute_reply":"2024-12-26T03:30:52.550372Z"}},"outputs":[{"name":"stdout","text":"{'verbose': True, 'with_cuda': True, 'extra_ldflags': ['-L/usr/local/cuda/lib', '-lcublas'], 'extra_cflags': ['-DSLSTM_HIDDEN_SIZE=128', '-DSLSTM_BATCH_SIZE=8', '-DSLSTM_NUM_HEADS=8', '-DSLSTM_NUM_STATES=4', '-DSLSTM_DTYPE_B=float', '-DSLSTM_DTYPE_R=__nv_bfloat16', '-DSLSTM_DTYPE_W=__nv_bfloat16', '-DSLSTM_DTYPE_G=__nv_bfloat16', '-DSLSTM_DTYPE_S=__nv_bfloat16', '-DSLSTM_DTYPE_A=float', '-DSLSTM_NUM_GATES=4', '-DSLSTM_SIMPLE_AGG=true', '-DSLSTM_GRADIENT_RECURRENT_CLIPVAL_VALID=false', '-DSLSTM_GRADIENT_RECURRENT_CLIPVAL=0.0', '-DSLSTM_FORWARD_CLIPVAL_VALID=false', '-DSLSTM_FORWARD_CLIPVAL=0.0', '-U__CUDA_NO_HALF_OPERATORS__', '-U__CUDA_NO_HALF_CONVERSIONS__', '-U__CUDA_NO_BFLOAT16_OPERATORS__', '-U__CUDA_NO_BFLOAT16_CONVERSIONS__', '-U__CUDA_NO_BFLOAT162_OPERATORS__', '-U__CUDA_NO_BFLOAT162_CONVERSIONS__'], 'extra_cuda_cflags': ['-Xptxas=\"-v\"', '-gencode', 'arch=compute_80,code=compute_80', '-res-usage', '--use_fast_math', '-O3', '-Xptxas -O3', '--extra-device-vectorization', '-DSLSTM_HIDDEN_SIZE=128', '-DSLSTM_BATCH_SIZE=8', '-DSLSTM_NUM_HEADS=8', '-DSLSTM_NUM_STATES=4', '-DSLSTM_DTYPE_B=float', '-DSLSTM_DTYPE_R=__nv_bfloat16', '-DSLSTM_DTYPE_W=__nv_bfloat16', '-DSLSTM_DTYPE_G=__nv_bfloat16', '-DSLSTM_DTYPE_S=__nv_bfloat16', '-DSLSTM_DTYPE_A=float', '-DSLSTM_NUM_GATES=4', '-DSLSTM_SIMPLE_AGG=true', '-DSLSTM_GRADIENT_RECURRENT_CLIPVAL_VALID=false', '-DSLSTM_GRADIENT_RECURRENT_CLIPVAL=0.0', '-DSLSTM_FORWARD_CLIPVAL_VALID=false', '-DSLSTM_FORWARD_CLIPVAL=0.0', '-U__CUDA_NO_HALF_OPERATORS__', '-U__CUDA_NO_HALF_CONVERSIONS__', '-U__CUDA_NO_BFLOAT16_OPERATORS__', '-U__CUDA_NO_BFLOAT16_CONVERSIONS__', '-U__CUDA_NO_BFLOAT162_OPERATORS__', '-U__CUDA_NO_BFLOAT162_CONVERSIONS__']}\n","output_type":"stream"},{"name":"stderr","text":"Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\nCreating extension directory /root/.cache/torch_extensions/py310_cu121/slstm_HS128BS8NH8NS4DBfDRbDWbDGbDSbDAfNG4SA1GRCV0GRC0d0FCV0FC0d0...\nDetected CUDA files, patching ldflags\nEmitting ninja build file /root/.cache/torch_extensions/py310_cu121/slstm_HS128BS8NH8NS4DBfDRbDWbDGbDSbDAfNG4SA1GRCV0GRC0d0FCV0FC0d0/build.ninja...\n/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \nIf this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n  warnings.warn(\nBuilding extension module slstm_HS128BS8NH8NS4DBfDRbDWbDGbDSbDAfNG4SA1GRCV0GRC0d0FCV0FC0d0...\nAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\nLoading extension module slstm_HS128BS8NH8NS4DBfDRbDWbDGbDSbDAfNG4SA1GRCV0GRC0d0FCV0FC0d0...\n/usr/local/lib/python3.10/dist-packages/xlstm/blocks/slstm/cell.py:546: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  def forward(ctx, training, *inputs):\n/usr/local/lib/python3.10/dist-packages/xlstm/blocks/slstm/cell.py:571: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  def backward(ctx, grad_s):\n","output_type":"stream"},{"name":"stdout","text":"The model has 8,477,552 trainable parameters\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## Train","metadata":{}},{"cell_type":"code","source":"def train(model, data, optimizer, criterion, batch_size, seq_len, clip, device):\n    \n    epoch_loss = 0\n    model.train()\n    # drop all batches that are not a multiple of seq_len\n    num_batches = data.shape[-1]\n    data = data[:, :num_batches - (num_batches -1) % seq_len]\n    num_batches = data.shape[-1]\n\n    hidden = None  # xLSTM does not require explicit hidden state initialization\n    \n    for idx in tqdm(range(0, num_batches - 1, seq_len), desc='Training: ',leave=False):  # The last batch can't be a src\n        optimizer.zero_grad()\n\n        src, target = get_batch(data, seq_len, num_batches, idx)\n        src, target = src.to(device), target.to(device)\n        batch_size = src.shape[0]\n        prediction = model(src)                 # model output\n\n        prediction = prediction.reshape(batch_size * seq_len, -1)   \n        target = target.reshape(-1)\n        loss = criterion(prediction, target)\n        \n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n        optimizer.step()\n        epoch_loss += loss.item() * seq_len\n    return epoch_loss / num_batches\ndef evaluate(model, data, criterion, batch_size, seq_len, device):\n\n    epoch_loss = 0\n    model.eval()\n    num_batches = data.shape[-1]\n    data = data[:, :num_batches - (num_batches -1) % seq_len]\n    num_batches = data.shape[-1]\n\n    hidden = None  # xLSTM does not require explicit hidden state initialization\n\n    with torch.no_grad():\n        for idx in range(0, num_batches - 1, seq_len):\n            src, target = get_batch(data, seq_len, num_batches, idx)\n            src, target = src.to(device), target.to(device)\n            batch_size= src.shape[0]\n\n            prediction = model(src)\n            prediction = prediction.reshape(batch_size * seq_len, -1)\n            target = target.reshape(-1)\n\n            loss = criterion(prediction, target)\n            epoch_loss += loss.item() * seq_len\n    return epoch_loss / num_batches\nn_epochs = 50\nseq_len = 50\nclip = 0.25\nsaved = False\n\nlr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=0)\n\nif saved:\n    model.load_state_dict(torch.load('best-val-xlstm_lm.pt',  map_location=device))\n    test_loss = evaluate(model, test_data, criterion, batch_size, seq_len, device)\n    print(f'Test Perplexity: {math.exp(test_loss):.3f}')\nelse:\n    best_valid_loss = float('inf')\n\n    for epoch in range(n_epochs):\n        print(f\"Epoch {epoch+1}:\")\n        train_loss = train(model, train_data, optimizer, criterion, batch_size, seq_len, clip, device)\n        valid_loss = evaluate(model, valid_data, criterion, batch_size, seq_len, device)\n        \n        lr_scheduler.step(valid_loss)\n\n        if valid_loss < best_valid_loss:\n            best_valid_loss = valid_loss\n            torch.save(model.state_dict(), 'best-xlstm.pt')\n\n        print(f'\\tTrain Perplexity: {math.exp(train_loss):.3f}')\n        print(f'\\tValid Perplexity: {math.exp(valid_loss):.3f}')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T03:30:56.112513Z","iopub.execute_input":"2024-12-26T03:30:56.112809Z","iopub.status.idle":"2024-12-26T04:09:13.522008Z","shell.execute_reply.started":"2024-12-26T03:30:56.112786Z","shell.execute_reply":"2024-12-26T04:09:13.521243Z"}},"outputs":[{"name":"stdout","text":"Epoch 1:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 690.731\n\tValid Perplexity: 283.054\nEpoch 2:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 257.716\n\tValid Perplexity: 208.917\nEpoch 3:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 184.550\n\tValid Perplexity: 179.101\nEpoch 4:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 145.117\n\tValid Perplexity: 163.054\nEpoch 5:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 120.086\n\tValid Perplexity: 155.606\nEpoch 6:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 102.757\n\tValid Perplexity: 151.040\nEpoch 7:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 90.045\n\tValid Perplexity: 149.421\nEpoch 8:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 80.145\n\tValid Perplexity: 150.094\nEpoch 9:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 69.643\n\tValid Perplexity: 146.796\nEpoch 10:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 65.249\n\tValid Perplexity: 148.449\nEpoch 11:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 60.433\n\tValid Perplexity: 147.769\nEpoch 12:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 57.851\n\tValid Perplexity: 147.175\nEpoch 13:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 56.581\n\tValid Perplexity: 147.071\nEpoch 14:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 55.954\n\tValid Perplexity: 146.950\nEpoch 15:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 55.470\n\tValid Perplexity: 146.838\nEpoch 16:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 55.169\n\tValid Perplexity: 146.700\nEpoch 17:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 55.068\n\tValid Perplexity: 146.717\nEpoch 18:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 54.928\n\tValid Perplexity: 146.640\nEpoch 19:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 54.851\n\tValid Perplexity: 146.605\nEpoch 20:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 54.829\n\tValid Perplexity: 146.597\nEpoch 21:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 54.791\n\tValid Perplexity: 146.594\nEpoch 22:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 54.771\n\tValid Perplexity: 146.594\nEpoch 23:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 54.761\n\tValid Perplexity: 146.594\nEpoch 24:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 54.756\n\tValid Perplexity: 146.594\nEpoch 25:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 54.753\n\tValid Perplexity: 146.594\nEpoch 26:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 54.752\n\tValid Perplexity: 146.594\nEpoch 27:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 54.752\n\tValid Perplexity: 146.594\nEpoch 28:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 54.751\n\tValid Perplexity: 146.593\nEpoch 29:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 54.751\n\tValid Perplexity: 146.593\nEpoch 30:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 54.751\n\tValid Perplexity: 146.593\nEpoch 31:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 54.751\n\tValid Perplexity: 146.593\nEpoch 32:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 54.751\n\tValid Perplexity: 146.593\nEpoch 33:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 54.751\n\tValid Perplexity: 146.593\nEpoch 34:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 54.751\n\tValid Perplexity: 146.593\nEpoch 35:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 54.751\n\tValid Perplexity: 146.593\nEpoch 36:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 54.750\n\tValid Perplexity: 146.593\nEpoch 37:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 54.750\n\tValid Perplexity: 146.593\nEpoch 38:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 54.750\n\tValid Perplexity: 146.593\nEpoch 39:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 54.750\n\tValid Perplexity: 146.592\nEpoch 40:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 54.750\n\tValid Perplexity: 146.592\nEpoch 41:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 54.750\n\tValid Perplexity: 146.592\nEpoch 42:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 54.750\n\tValid Perplexity: 146.592\nEpoch 43:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 54.750\n\tValid Perplexity: 146.592\nEpoch 44:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 54.749\n\tValid Perplexity: 146.592\nEpoch 45:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 54.749\n\tValid Perplexity: 146.592\nEpoch 46:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 54.749\n\tValid Perplexity: 146.592\nEpoch 47:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 54.749\n\tValid Perplexity: 146.592\nEpoch 48:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 54.749\n\tValid Perplexity: 146.592\nEpoch 49:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 54.749\n\tValid Perplexity: 146.592\nEpoch 50:\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 54.749\n\tValid Perplexity: 146.592\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## Test\n","metadata":{}},{"cell_type":"code","source":"model.load_state_dict(torch.load('/kaggle/working/best-xlstm.pt',  map_location=device))\ntest_loss = evaluate(model, test_data, criterion, batch_size, seq_len, device)\nprint(f'Test Perplexity: {math.exp(test_loss):.3f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T04:16:05.203215Z","iopub.execute_input":"2024-12-26T04:16:05.203561Z","iopub.status.idle":"2024-12-26T04:16:06.557559Z","shell.execute_reply.started":"2024-12-26T04:16:05.203534Z","shell.execute_reply":"2024-12-26T04:16:06.556669Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-9-201b0ae4460d>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('/kaggle/working/best-xlstm.pt',  map_location=device))\n","output_type":"stream"},{"name":"stdout","text":"Test Perplexity: 137.482\n","output_type":"stream"}],"execution_count":9}]}